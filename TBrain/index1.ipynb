{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "---\n",
    "1. [Detecting Parkinson’s Disease with OpenCV, Computer Vision, and the Spiral/Wave Test](https://www.pyimagesearch.com/2019/04/29/detecting-parkinsons-disease-with-opencv-computer-vision-and-the-spiral-wave-test/)\n",
    "- [Distinguishing Different Stages of Parkinson’s Disease Using Composite Index of Speed and Pen-Pressure of Sketching a Spiral, by Zham et al.](https://www.frontiersin.org/articles/10.3389/fneur.2017.00435/full), The researchers found that the drawing speed was slower and the pen pressure lower among Parkinson’s patients.\n",
    "- [Images Dataset, NIATS of Federal University of Uberlândia.](http://www.niats.feelt.ufu.br/en/node/81)\n",
    "- [Histogram of Oriented Gradients for Human Detection](https://ieeexplore.ieee.org/document/1467360), HOG is a structural descriptor that will capture and quantify changes in local gradient in the input image. HOG will naturally be able to quantify how the directions of a both spirals and waves change. And furthermore, HOG will be able to capture if these drawings have more of a “shake” to them, as we might expect from a Parkinson’s patient.\n",
    "\n",
    "\n",
    "Parkinson’s disease is a nervous system disorder that affects movement. The disease is progressive and is marked by five different stages (source).\n",
    "\n",
    "- Stage 1: Mild symptoms that do not typically interfere with daily life, including tremors and movement issues on only one side of the body.\n",
    "- Stage 2: Symptoms continue to become worse with both tremors and rigidity now affecting both sides of the body. Daily tasks become challenging.\n",
    "- Stage 3: Loss of balance and movements with falls becoming frequent and common. The patient is still capable of (typically) living independently.\n",
    "- Stage 4: Symptoms become severe and constraining. The patient is unable to live alone and requires help to perform daily activities.\n",
    "- Stage 5: Likely impossible to walk or stand. The patient is most likely wheelchair bound and may even experience hallucinations.\n",
    "\n",
    "While Parkinson’s cannot be cured, early detection along with proper medication can significantly improve symptoms and quality of life, making it an important topic as computer vision and machine learning practitioners to explore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introtroduction\n",
    "---\n",
    "A 2017 study by [Zham et al.](https://www.frontiersin.org/articles/10.3389/fneur.2017.00435/full) found that it was possible to detect Parkinson’s by asking the patient to draw a spiral and then track:\n",
    "\n",
    "1. Speed of drawing\n",
    "- Pen pressure\n",
    "\n",
    "The researchers found that the drawing speed was slower and the pen pressure lower among Parkinson’s patients — this was especially pronounced for patients with a more acute/advanced forms of the disease.\n",
    "\n",
    "**Dataset** \n",
    "is availed by  [Adriano de Oliveira Andrade and Joao Paulo Folado from the NIATS of Federal University of Uberlândia](http://www.niats.feelt.ufu.br/en/node/81), which consists of 204 images and is pre-split into a training set and a testing set, consisting of:\n",
    "\n",
    "1. Spiral: 102 images, 72 training, and 30 testing\n",
    "- Wave: 102 images, 72 training, and 30 testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/70/33f2757428cff754ade0d4433e39e5dabc0c7d0359190eafba97676f5129/scikit_learn-0.21.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (10.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.5MB 1.1MB/s ta 0:00:01    97% |███████████████████████████████ | 10.2MB 837kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 7.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.15.4)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Found existing installation: scikit-learn 0.20.3\n",
      "    Uninstalling scikit-learn-0.20.3:\n",
      "      Successfully uninstalled scikit-learn-0.20.3\n",
      "Successfully installed joblib-0.13.2 scikit-learn-0.21.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn #加-U才會安裝到最新版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SciPy in /anaconda3/lib/python3.7/site-packages (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/0c/659c2bdae8e8ca5ef810b9da02db28feaa29ea448ff36b65a1664ff28142/imutils-0.5.2.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Running setup.py bdist_wheel for imutils ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/francesca/Library/Caches/pip/wheels/b2/40/59/139d450e68847ef2f27d876d527b13389dac23df0f66526b5d\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import feature,exposure\n",
    "from imutils import build_montages #蒙太奇（把影像做變形）\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "import cv2 #用來轉換image成information\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HOG](https://ieeexplore.ieee.org/document/1467360) is a structural descriptor that will capture and quantify changes in local gradient in the input image. HOG will naturally be able to quantify how the directions of a both spirals and waves change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.hog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_image(image):\n",
    "     # compute the histogram of oriented gradients feature vector for\n",
    "     # the input image\n",
    "     features = feature.hog(image, orientations=9,\n",
    "          pixels_per_cell=(10, 10), cells_per_block=(2, 2),\n",
    "          transform_sqrt=True, block_norm=\"L1\")\n",
    "\n",
    "     # return the feature vector\n",
    "     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-34323796d752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset/spiral/testing/parkinson/V01PE01.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.0.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "img1=\"dataset/spiral/testing/healthy/V01HE01.png\"\n",
    "img2=\"dataset/spiral/testing/parkinson/V01PE01.png\"\n",
    "img = cv2.imread(img1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.resize(img, (200, 200))\n",
    "ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "ax.imshow(img)\n",
    "img0 = cv2.imread(img2)\n",
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)\n",
    "img0 = cv2.resize(img0, (200, 200))\n",
    "ax = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "ax.imshow(img0)\n",
    "plt.xticks([]), plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fa0e93ab6154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m image = cv2.threshold(img, 0, 255,\n\u001b[1;32m      4\u001b[0m                cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2701\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    640\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    641\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data cannot be converted to float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The thresholding step segments the drawing from the input image, \n",
    "# making the drawing appear as white foreground on a black background\n",
    "image = cv2.threshold(img, 0, 255,\n",
    "               cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify the image by 12996-features\n",
    "features = quantify_image(image)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, hogImage) = feature.hog(image, orientations=9, pixels_per_cell=(10, 10),\n",
    "                   cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\",\n",
    "                   visualise=True)\n",
    "hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))\n",
    "hogImage = hogImage.astype(\"uint8\")\n",
    " \n",
    "plt.imshow(hogImage)\n",
    "plt.xticks([]), plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(path):\n",
    "     # grab the list of images in the input directory, then initialize\n",
    "     # the list of data (i.e., images) and class labels\n",
    "     imagePaths = list(paths.list_images(path))\n",
    "     data = []\n",
    "     labels = []\n",
    "\n",
    "     # loop over the image paths\n",
    "     for imagePath in tqdm(imagePaths):\n",
    "          # extract the class label from the filename\n",
    "          label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "          # load the input image, convert it to grayscale, and resize\n",
    "          # it to 200x200 pixels, ignoring aspect ratio\n",
    "          image = cv2.imread(imagePath)\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "          image = cv2.resize(image, (200, 200))\n",
    "\n",
    "          # threshold the image such that the drawing appears as white\n",
    "          # on a black background\n",
    "          image = cv2.threshold(image, 0, 255,\n",
    "               cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "          # quantify the image\n",
    "          features = quantify_image(image)\n",
    "\n",
    "          # update the data and labels lists, respectively\n",
    "          data.append(features)\n",
    "          labels.append(label)\n",
    "\n",
    "     # return the data and labels\n",
    "     return (np.array(data), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the training and testing directories\n",
    "trainingPath =\"dataset/spiral/training\"\n",
    "testingPath = \"dataset/spiral/testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading data...\")\n",
    "(trainX, trainY) = load_split(trainingPath)\n",
    "(testX, testY) = load_split(testingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels as integers\n",
    "le = LabelEncoder()\n",
    "trainY = le.fit_transform(trainY)\n",
    "testY = le.transform(testY)\n",
    "\n",
    "# initialize our trials dictionary\n",
    "trials = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "BGBC=HistGradientBoostingClassifier(learning rate=0.01,min_samples_leaf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the number of trials to run\n",
    "ntrials=5\n",
    "for i in tqdm_notebook(range(0, ntrials)):\n",
    "     # train the model\n",
    "     print(\"[INFO] training model {} of {}...\".format(i + 1,ntrials))\n",
    "     model = RandomForestClassifier(n_estimators=100)\n",
    "     model.fit(trainX, trainY)\n",
    "\n",
    "     # make predictions on the testing data and initialize a dictionary\n",
    "     # to store our computed metrics\n",
    "     predictions = model.predict(testX)\n",
    "     metrics = {}\n",
    "\n",
    "     # compute the confusion matrix and and use it to derive the raw\n",
    "     # accuracy, sensitivity, and specificity\n",
    "     cm = confusion_matrix(testY, predictions).flatten()\n",
    "     (tn, fp, fn, tp) = cm\n",
    "     metrics[\"acc\"] = (tp + tn) / float(cm.sum())\n",
    "     metrics[\"sensitivity\"] = tp / float(tp + fn)\n",
    "     metrics[\"specificity\"] = tn / float(tn + fp)\n",
    "\n",
    "     # loop over the metrics\n",
    "     for (k, v) in metrics.items():\n",
    "          # update the trials dictionary with the list of values for\n",
    "          # the current metric\n",
    "          l = trials.get(k, [])\n",
    "          l.append(v)\n",
    "          trials[k] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmo=confusion_matrix(testY, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df_cm = pd.DataFrame(cmo, range(2),\n",
    "                  range(2))\n",
    "#plt.figure(figsize = (10,7))\n",
    "\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over our metrics\n",
    "for metric in (\"acc\", \"sensitivity\", \"specificity\"):\n",
    "     # grab the list of values for the current metric, then compute\n",
    "     # the mean and standard deviation\n",
    "     values = trials[metric]\n",
    "     mean = np.mean(values)\n",
    "     std = np.std(values)\n",
    "\n",
    "     # show the computed metrics for the statistic\n",
    "     print(metric)\n",
    "     print(\"=\" * len(metric))\n",
    "     print(\"𝝁={:.4f}, 𝝈={:.4f}\".format(mean, std))\n",
    "     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a few images and then initialize the output images\n",
    "# for the montage\n",
    "testingPaths = list(paths.list_images(testingPath))\n",
    "idxs = np.arange(0, len(testingPaths))\n",
    "idxs = np.random.choice(idxs, size=(25,), replace=False)\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the testing samples\n",
    "for i in idxs:\n",
    "     # load the testing image, clone it, and resize it\n",
    "     image = cv2.imread(testingPaths[i])\n",
    "     output = image.copy()\n",
    "     output = cv2.resize(output, (128, 128))\n",
    "\n",
    "     # pre-process the image in the same manner we did earlier\n",
    "     image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "     image = cv2.resize(image, (200, 200))\n",
    "     image = cv2.threshold(image, 0, 255,\n",
    "          cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "     # quantify the image and make predictions based on the extracted\n",
    "     # features using the last trained Random Forest\n",
    "     features = quantify_image(image)\n",
    "     preds = model.predict([features])\n",
    "     label = le.inverse_transform(preds)[0]\n",
    "\n",
    "     # draw the colored class label on the output image and add it to\n",
    "     # the set of output images\n",
    "     color = (255, 0, 0) if label == 'parkinson' else (0, 0, 255)\n",
    "       \n",
    "     cv2.putText(output, label, (3, 20),cv2.FONT_HERSHEY_SIMPLEX , 0.5,color, 2)\n",
    "     images.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a montage using 128x128 \"tiles\" with 5 rows and 5 columns\n",
    "montage = build_montages(images, (128, 128), (5, 5))[0]\n",
    "\n",
    "# show the output montage\n",
    "cv2.imshow(\"Output\", montage)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(montage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
