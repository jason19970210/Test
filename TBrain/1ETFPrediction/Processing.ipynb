{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Steps\n",
    "---\n",
    "1. [download new data, concatenate data](fbprophet-Introduction.ipynb)\n",
    "   - [Dive new Features, US Change, etc](USChanges.ipynb)\n",
    "   - [Noises ?]()\n",
    "- [Forceast data for the next week, finalize data, and output result](GreyBox-ind2-xgb)\n",
    "- [ToDo, pack models](...)\n",
    "\n",
    "\n",
    "Reference\n",
    "---\n",
    "1. [Stock Prediction](Python-Machine-Learning-By-Example-master_chap07/Chapter07/1stock_price_prediction.py)\n",
    "- [FFT prediction](Test-stock-prediction-algorithms-master/Misc%20experiments/FFT_Stock_Prediction.py)\n",
    "- [Volatility](Test-stock-prediction-algorithms-master/Misc%20experiments/Volatility.py)\n",
    "\n",
    "Note\n",
    "---\n",
    "1. [Data length](#Length-Calculation)\n",
    "- [Prediction Vis](#Prediction-Visualization)\n",
    "\n",
    "\n",
    "Data\n",
    "---\n",
    "1. data1 (orig), yahoo/20180518 (new),  yahoo/update20180518 (features added)\n",
    "-  yahoo/20180518pre, yahoo/20180518pre30\n",
    "-  yahoo//YahooFinance, (Market Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib,os\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "# load Lasso\n",
    "from sklearn.linear_model import  Lasso ,MultiTaskElasticNet\n",
    "# load NN functions\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Activation\n",
    "\n",
    "from xgboost import XGBRegressor,plot_importance\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time\n",
    "today=time.strftime(\"%Y-%m-%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USind={'^IXIC':'NDAQ','^RUT':'Russel2000','^GSPC':'SandP','GC=F':'Gold','CL=F':'Oil','^VIX':'VIX','^DJA':'DJA','^DJI':'DJI',\n",
    "       'DX-Y.NYB':'USDInx'}\n",
    "US_indecies=['^IXIC','^RUT','^GSPC','GC=F','CL=F','^VIX','^DJA','^DJI','DX-Y.NYB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create US Stock market dataframe, df_yahoo\n",
    "\n",
    "df_yahoo = pd.DataFrame()\n",
    "for US_index in US_indecies:\n",
    "    dfname=\"yahoo/YahooFinance/%s.csv\" %USind[US_index]\n",
    "    featurename='%s' %USind[US_index]\n",
    "    #print(dfname)\n",
    "    df_tmp=pd.read_csv(dfname,index_col='Date') \n",
    "    df_yahoo[featurename]=df_tmp.Close\n",
    "    \n",
    "    del df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date=pd.read_csv(\"yahoo/20180518pre/0050.csv\",index_col='Date') \n",
    "df0050_date.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date_1=df0050_date.copy()\n",
    "stock_df = Sdf.retype(df0050_date_1)\n",
    "#df0050_date['rsi5']=stock_df['rsi_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volatility\n",
    "---\n",
    "\n",
    "1. $\\mathbf{\\triangle x_i= \\log x_i -\\log x_{i-1}}$, the return of $x_i$\n",
    "-  Volatility=$\\mathbf{\\sum_w \\sigma(\\triangle x_i)/w}$, where the window, $w$, is the length of considered period, and $\\sigma$ is the (positive standard deviation.\n",
    "\n",
    "**Robert L. McDonald**, Derivatives Markets (3rd Edition) (Pearson Series in Finance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "If no data from US market, comment out features creation df_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_p23(df):\n",
    "    \"\"\" Generate features for a stock/index based on historical price and performance\n",
    "    Args:\n",
    "        df (dataframe with columns \"Open\", \"Close\", \"High\", \"Low\", \"Volume\", \"Adjusted Close\")\n",
    "    Returns:\n",
    "        dataframe, data set with new features\n",
    "    \"\"\"\n",
    "    df_new = pd.DataFrame()\n",
    "    # 6 original features\n",
    "    df_new['High'] = df['High']\n",
    "    df_new['Low'] = df['Low']\n",
    "    df.Date=df.index\n",
    "    df_new['weekday']=pd.DatetimeIndex(df.Date).dayofweek.values\n",
    "    df_new['weekday']=df_new['weekday'].astype('int')\n",
    "    #df_new['weekday'] = df['weekday']\n",
    "    #df_new['code'] = df['code']\n",
    "    #df_new['price_change'] = df['price_Change']\n",
    "    #df_new['sign'] = df['sign']\n",
    "    \n",
    "    \n",
    "    df_new['Open'] = df['Open']\n",
    "    df_new['open_1'] = df['Open'].shift(1)\n",
    "    df_new['close_1'] = df['Close'].shift(1)\n",
    "    df_new['high_1'] = df['High'].shift(1)\n",
    "    df_new['low_1'] = df['Low'].shift(1)\n",
    "    df_new['volume_1'] = df['Volume'].shift(1)\n",
    "    \n",
    "    # 31 original features\n",
    "\n",
    "    # rolling_mean desprecated since pandas 0.23\n",
    "    # use df.rolling(windows).mean().shift(1) in placed\n",
    "    #      df.rolling(windows).std().shift(1)\n",
    "    \n",
    "        # average price\n",
    "    df_new['avg_price_5'] = df['Close'].rolling(5).mean().shift(1)\n",
    "    df_new['avg_price_30'] = df['Close'].rolling(21).mean().shift(1)\n",
    "    df_new['avg_price_365'] = df['Close'].rolling(252).mean().shift(1)\n",
    "    df_new['ratio_avg_price_5_30'] = df_new['avg_price_5'] / df_new['avg_price_30']\n",
    "    df_new['ratio_avg_price_5_365'] = df_new['avg_price_5'] / df_new['avg_price_365']\n",
    "    df_new['ratio_avg_price_30_365'] = df_new['avg_price_30'] / df_new['avg_price_365']\n",
    "    \n",
    "    \n",
    "    # average volume\n",
    "    df_new['avg_volume_5'] = df['Volume'].rolling(5).mean().shift(1)\n",
    "    df_new['avg_volume_30'] = df['Volume'].rolling(21).mean().shift(1)\n",
    "    df_new['avg_volume_365'] = df['Volume'].rolling(252).mean().shift(1)\n",
    "    df_new['ratio_avg_volume_5_30'] = df_new['avg_volume_5'] / df_new['avg_volume_30']\n",
    "    df_new['ratio_avg_volume_5_365'] = df_new['avg_volume_5'] / df_new['avg_volume_365']\n",
    "    df_new['ratio_avg_volume_30_365'] = df_new['avg_volume_30'] / df_new['avg_volume_365']\n",
    "    \n",
    "    # standard deviation of prices\n",
    "    \n",
    "    df_new['std_price_5'] = df['Close'].rolling(5).std().shift(1)\n",
    "    df_new['std_price_30'] = df['Close'].rolling(21).std().shift(1)\n",
    "    df_new['std_price_365'] = df['Close'].rolling(252).std().shift(1)\n",
    "    df_new['ratio_std_price_5_30'] = df_new['std_price_5'] / df_new['std_price_30']\n",
    "    df_new['ratio_std_price_5_365'] = df_new['std_price_5'] / df_new['std_price_365']\n",
    "    df_new['ratio_std_price_30_365'] = df_new['std_price_30'] / df_new['std_price_365']\n",
    "    \n",
    "    # standard deviation of volumes\n",
    "    df_new['std_volume_5'] = df['Volume'].rolling(5).std().shift(1)\n",
    "    df_new['std_volume_30'] = df['Volume'].rolling(21).std().shift(1)\n",
    "    df_new['std_volume_365'] = df['Volume'].rolling(252).std().shift(1)\n",
    "    df_new['ratio_std_volume_5_30'] = df_new['std_volume_5'] / df_new['std_volume_30']\n",
    "    df_new['ratio_std_volume_5_365'] = df_new['std_volume_5'] / df_new['std_volume_365']\n",
    "    df_new['ratio_std_volume_30_365'] = df_new['std_volume_30'] / df_new['std_volume_365']\n",
    "    # # return\n",
    "    df_new['return_1'] = ((df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1)).shift(1)\n",
    "    df_new['return_5'] = ((df['Close'] - df['Close'].shift(5)) / df['Close'].shift(5)).shift(1)\n",
    "    df_new['return_30'] = ((df['Close'] - df['Close'].shift(21)) / df['Close'].shift(21)).shift(1)\n",
    "    df_new['return_365'] = ((df['Close'] - df['Close'].shift(252)) / df['Close'].shift(252)).shift(1)\n",
    "    \n",
    "    df_new['return_1'].rolling(5).mean()\n",
    "    df_new['moving_avg_5'] = df_new['return_1'].rolling(5).mean()\n",
    "    df_new['moving_avg_30'] =df_new['return_1'].rolling(21).mean()\n",
    "    df_new['moving_avg_365'] = df_new['return_1'].rolling(252).mean()\n",
    "    \n",
    "    df_new['Log Ret'] = np.log(df['Close']/df['Close'].shift(1))\n",
    "    w = 63 # ~ 1 year\n",
    "    df_new['VolatilityQuater'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    w = 6\n",
    "    df_new['VolatilityWeek'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    w=28\n",
    "    df_new['VolatilityMonth'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    #w=252\n",
    "    #df_new['VolatilityYear'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    \n",
    "    df_US=pd.read_csv(\"USChange.csv\",index_col='Date') \n",
    "    df_new['US']=df_US.Close\n",
    "    \n",
    "    df_US['yesterday']=0\n",
    "    df_US['yesterday'][1:]=df_US.Close[1:]\n",
    "    df_US['US_Change']=df_US.Close-df_US.yesterday\n",
    "    df_new['US_Change']=df_US.US_Change\n",
    "    \n",
    "    del df_US\n",
    "    \n",
    "    #RSI\n",
    "       \n",
    "    df_1=df.copy()\n",
    "    stock_df = Sdf.retype(df_1)\n",
    "    ## reset Index \n",
    "    #stock_df.index=pd.to_datetime(stock_df.index, format='%Y%m%d')\n",
    "    #stock_df.index.names = ['Date']\n",
    "    \n",
    "    #print(stock_df['rsi_6'].tail())\n",
    "    df_new['RSI6']=stock_df['rsi_6'].values\n",
    "    df_new['RSI12']=stock_df['rsi_12'].values\n",
    "    df_new['RSI24']=stock_df['rsi_24'].values\n",
    "    df_new['VR']=stock_df['vr'].values\n",
    "    df_new['VR_6_sma']=stock_df['vr_6_sma'].values\n",
    "    df_new['volume_delta']=stock_df['volume_delta'].values\n",
    "    df_new['open_2_d']=stock_df['open_2_d'].values\n",
    "    df_new['open_-2_r']=stock_df['open_-2_r'].values\n",
    "    df_new['cr']=stock_df['cr'].values\n",
    "    df_new['cr-ma1']=stock_df['cr-ma1'].values\n",
    "    df_new['cr-ma2']=stock_df['cr-ma2'].values\n",
    "    df_new['cr-ma3']=stock_df['cr-ma3'].values\n",
    "    df_new['kdjk']=stock_df['kdjk'].values\n",
    "    df_new['kdjd']=stock_df['kdjd'].values\n",
    "    df_new['kdjj']=stock_df['kdjj'].values\n",
    "    df_new['macd']=stock_df['macd'].values\n",
    "    \n",
    "    # data from US stock market\n",
    "    \n",
    "    for US_index in US_indecies:       \n",
    "        featurename='%s' %USind[US_index]\n",
    "        df_name=\"yahoo/YahooFinance/%s.csv\" %USind[US_index]\n",
    "        df_yahoo=pd.read_csv(df_name,index_col='Date') \n",
    "        df_new[featurename]=df_yahoo['Close']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # the target\n",
    "    df_new['Close'] = df['Close']\n",
    "    df_new=df_new.fillna(method='ffill')\n",
    "    df_new=df_new.fillna(method='bfill')\n",
    "    #df_new = df_new.dropna(axis=0)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_p23_1(df):\n",
    "    \"\"\" Generate features for a stock/index based on historical price and performance\n",
    "    Args:\n",
    "        df (dataframe with columns \"Open\", \"Close\", \"High\", \"Low\", \"Volume\", \"Adjusted Close\")\n",
    "    Returns:\n",
    "        dataframe, data set with new features\n",
    "    \"\"\"\n",
    "    df_new = pd.DataFrame()\n",
    "    # 6 original features\n",
    "    df_new['High'] = df['High']\n",
    "    df_new['Low'] = df['Low']\n",
    "    df.Date=df.index\n",
    "    df_new['weekday']=pd.DatetimeIndex(df.Date).dayofweek.values\n",
    "    df_new['weekday']=df_new['weekday'].astype('int')\n",
    "    #df_new['weekday'] = df['weekday']\n",
    "    #df_new['code'] = df['code']\n",
    "    #df_new['price_change'] = df['price_Change']\n",
    "    #df_new['sign'] = df['sign']\n",
    "    \n",
    "    \n",
    "    df_new['Open'] = df['Open']\n",
    "    df_new['open_1'] = df['Open'].shift(1)\n",
    "    df_new['close_1'] = df['Close'].shift(1)\n",
    "    df_new['high_1'] = df['High'].shift(1)\n",
    "    df_new['low_1'] = df['Low'].shift(1)\n",
    "    df_new['volume_1'] = df['Volume'].shift(1)\n",
    "    \n",
    "    # 31 original features\n",
    "\n",
    "    # rolling_mean desprecated since pandas 0.23\n",
    "    # use df.rolling(windows).mean().shift(1) in placed\n",
    "    #      df.rolling(windows).std().shift(1)\n",
    "    \n",
    "        # average price\n",
    "    df_new['avg_price_5'] = df['Close'].rolling(5).mean().shift(1)\n",
    "    df_new['avg_price_30'] = df['Close'].rolling(21).mean().shift(1)\n",
    "    #df_new['avg_price_365'] = df['Close'].rolling(252).mean().shift(1)\n",
    "    df_new['ratio_avg_price_5_30'] = df_new['avg_price_5'] / df_new['avg_price_30']\n",
    "    #df_new['ratio_avg_price_5_365'] = df_new['avg_price_5'] / df_new['avg_price_365']\n",
    "    #df_new['ratio_avg_price_30_365'] = df_new['avg_price_30'] / df_new['avg_price_365']\n",
    "    \n",
    "    \n",
    "    # average volume\n",
    "    df_new['avg_volume_5'] = df['Volume'].rolling(5).mean().shift(1)\n",
    "    df_new['avg_volume_30'] = df['Volume'].rolling(21).mean().shift(1)\n",
    "    #df_new['avg_volume_365'] = df['Volume'].rolling(252).mean().shift(1)\n",
    "    df_new['ratio_avg_volume_5_30'] = df_new['avg_volume_5'] / df_new['avg_volume_30']\n",
    "    #df_new['ratio_avg_volume_5_365'] = df_new['avg_volume_5'] / df_new['avg_volume_365']\n",
    "    #df_new['ratio_avg_volume_30_365'] = df_new['avg_volume_30'] / df_new['avg_volume_365']\n",
    "    \n",
    "    # standard deviation of prices\n",
    "    \n",
    "    df_new['std_price_5'] = df['Close'].rolling(5).std().shift(1)\n",
    "    df_new['std_price_30'] = df['Close'].rolling(21).std().shift(1)\n",
    "    #df_new['std_price_365'] = df['Close'].rolling(252).std().shift(1)\n",
    "    df_new['ratio_std_price_5_30'] = df_new['std_price_5'] / df_new['std_price_30']\n",
    "    #df_new['ratio_std_price_5_365'] = df_new['std_price_5'] / df_new['std_price_365']\n",
    "    #df_new['ratio_std_price_30_365'] = df_new['std_price_30'] / df_new['std_price_365']\n",
    "    \n",
    "    # standard deviation of volumes\n",
    "    df_new['std_volume_5'] = df['Volume'].rolling(5).std().shift(1)\n",
    "    df_new['std_volume_30'] = df['Volume'].rolling(21).std().shift(1)\n",
    "    #df_new['std_volume_365'] = df['Volume'].rolling(252).std().shift(1)\n",
    "    df_new['ratio_std_volume_5_30'] = df_new['std_volume_5'] / df_new['std_volume_30']\n",
    "    #df_new['ratio_std_volume_5_365'] = df_new['std_volume_5'] / df_new['std_volume_365']\n",
    "    #df_new['ratio_std_volume_30_365'] = df_new['std_volume_30'] / df_new['std_volume_365']\n",
    "    # # return\n",
    "    df_new['return_1'] = ((df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1)).shift(1)\n",
    "    df_new['return_5'] = ((df['Close'] - df['Close'].shift(5)) / df['Close'].shift(5)).shift(1)\n",
    "    df_new['return_30'] = ((df['Close'] - df['Close'].shift(21)) / df['Close'].shift(21)).shift(1)\n",
    "    #df_new['return_365'] = ((df['Close'] - df['Close'].shift(252)) / df['Close'].shift(252)).shift(1)\n",
    "    \n",
    "    df_new['return_1'].rolling(5).mean()\n",
    "    df_new['moving_avg_5'] = df_new['return_1'].rolling(5).mean()\n",
    "    df_new['moving_avg_30'] =df_new['return_1'].rolling(21).mean()\n",
    "    #df_new['moving_avg_365'] = df_new['return_1'].rolling(252).mean()\n",
    "    \n",
    "    df_new['Log Ret'] = np.log(df['Close']/df['Close'].shift(1))\n",
    "    w = 63 # ~ 1 year\n",
    "    df_new['VolatilityQuater'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    w = 6\n",
    "    df_new['VolatilityWeek'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    w=28\n",
    "    df_new['VolatilityMonth'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    #w=252\n",
    "    #df_new['VolatilityYear'] = df_new['Log Ret'].rolling(window=w, center=True).std()\n",
    "    \n",
    "    df_US=pd.read_csv(\"USChange.csv\",index_col='Date') \n",
    "    df_new['US']=df_US.Close\n",
    "    \n",
    "    df_US['yesterday']=0\n",
    "    df_US['yesterday'][1:]=df_US.Close[1:]\n",
    "    df_US['US_Change']=df_US.Close-df_US.yesterday\n",
    "    df_new['US_Change']=df_US.US_Change\n",
    "    \n",
    "    del df_US\n",
    "    \n",
    "    #RSI\n",
    "       \n",
    "    df_1=df.copy()\n",
    "    stock_df = Sdf.retype(df_1)\n",
    "    ## reset Index \n",
    "    #stock_df.index=pd.to_datetime(stock_df.index, format='%Y%m%d')\n",
    "    #stock_df.index.names = ['Date']\n",
    "    \n",
    "    #print(stock_df['rsi_6'].tail())\n",
    "    df_new['RSI6']=stock_df['rsi_6'].values\n",
    "    df_new['RSI12']=stock_df['rsi_12'].values\n",
    "    df_new['RSI24']=stock_df['rsi_24'].values\n",
    "    df_new['VR']=stock_df['vr'].values\n",
    "    df_new['VR_6_sma']=stock_df['vr_6_sma'].values\n",
    "    df_new['volume_delta']=stock_df['volume_delta'].values\n",
    "    df_new['open_2_d']=stock_df['open_2_d'].values\n",
    "    df_new['open_-2_r']=stock_df['open_-2_r'].values\n",
    "    df_new['cr']=stock_df['cr'].values\n",
    "    df_new['cr-ma1']=stock_df['cr-ma1'].values\n",
    "    df_new['cr-ma2']=stock_df['cr-ma2'].values\n",
    "    df_new['cr-ma3']=stock_df['cr-ma3'].values\n",
    "    df_new['kdjk']=stock_df['kdjk'].values\n",
    "    df_new['kdjd']=stock_df['kdjd'].values\n",
    "    df_new['kdjj']=stock_df['kdjj'].values\n",
    "    df_new['macd']=stock_df['macd'].values\n",
    "    \n",
    "    # data from US stock market\n",
    "    \n",
    "    for US_index in US_indecies:       \n",
    "        featurename='%s' %USind[US_index]\n",
    "        df_name=\"yahoo/YahooFinance/%s.csv\" %USind[US_index]\n",
    "        df_yahoo=pd.read_csv(df_name,index_col='Date') \n",
    "        df_new[featurename]=df_yahoo['Close']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # the target\n",
    "    df_new['Close'] = df['Close']\n",
    "    df_new=df_new.fillna(method='ffill')\n",
    "    df_new=df_new.fillna(method='bfill')\n",
    "    df_new = df_new.dropna(axis=0)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=generate_features_p23_1(df0050_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data=generate_features(df_taihua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = list(data.drop(['Close'], axis=1).columns)\n",
    "y_column = 'Close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsceme(df,size=150):\n",
    "    \n",
    "    \n",
    "    data=generate_features_p23(df0050_date)\n",
    "    X_columns = list(data.drop(['Close'], axis=1).columns)\n",
    "    y_column = 'Close'\n",
    "    size=size\n",
    "    X = data[X_columns][-size:]\n",
    "    y = data[y_column][-size:]\n",
    "    \n",
    "    val_ratio =0.2\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=val_ratio,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data.as_matrix()\n",
    "#Y=data.Close\n",
    "size=150\n",
    "X = data[X_columns][-150:]\n",
    "y = data[y_column][-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_ratio =0.2\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=val_ratio,random_state=42)\n",
    "#X_test=X[-5:]\n",
    "#Y_test=y[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "---\n",
    "1. $\\mathbf{MSE}=\\frac{1}{n}\\sqrt{\\sum{(y_i-y^{prec}_i)^2}}$: small (~0) for good, big for bad (~1)\n",
    "- $\\mathbf{MAE}=\\frac{1}{n}{\\sum{\\left|y_i-y^{prec}_i\\right|}}$: small (~0) for good, big for bad (~1)\n",
    "- $\\mathbf{R^2}$: small (~0) for bad, big for good (~1)\n",
    "  - $\\mathbf{SST=\\text{Sum of tatal error}}=\\sum(y_i-\\bar y)^2$\n",
    "  - $\\mathbf{SSR=\\text{Sum of Residues}}=\\sum(y_i-y^{prec}_i)^2$\n",
    "  - $\\mathbf{R^2=1-\\frac{SSR}{SST}}$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# First experiment with linear regression\n",
    "\n",
    "# SGD is very sensitive to data with features at different scales. Hence we need to do feature scaling before training.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "X_scaled_val = scaler.transform(X_val)\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 3e-5, 1e-4],\n",
    "    \"eta0\": [0.01, 0.03, 0.1],\n",
    "}\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "lr = SGDRegressor(penalty='l2', n_iter=1000)\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_scaled_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "lr_best = grid_search.best_estimator_\n",
    "# print(grid_search.best_score_)\n",
    "\n",
    "predictions_lr = lr_best.predict(X_scaled_val)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "print('MSE: {0:.3f}'.format(mean_squared_error(y_val, predictions_lr)))\n",
    "print('MAE: {0:.3f}'.format(mean_absolute_error(y_val, predictions_lr)))\n",
    "print('R^2: {0:.3f}'.format(r2_score(y_val, predictions_lr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_test = scaler.transform(X_test)\n",
    "lr_best.predict(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date=pd.read_csv(\"yahoo/20180518/0050.csv\",index_col='Date') \n",
    "\n",
    "def gridsceme(df,size=150):\n",
    "    \n",
    "    \n",
    "    data=generate_features_p23(df0050_date)\n",
    "    X_columns = list(data.drop(['Close'], axis=1).columns)\n",
    "    y_column = 'Close'\n",
    "    size=size\n",
    "    X = data[X_columns][-size:]\n",
    "    y = data[y_column][-size:]\n",
    "    \n",
    "    val_ratio =0.2\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=val_ratio,random_state=42)\n",
    "    \n",
    "    param_grid = {\n",
    "       \"learning_rate\": [0.05,0.1,0.2],\n",
    "       \"max_depth\": [5, 10, 15, 20],\n",
    "       \"colsample_bytree\": [0.7,0.8,0.9],\n",
    "    }\n",
    "\n",
    "    model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)\n",
    "\n",
    "    grid_search = GridSearchCV(model_xgb, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "    # print(grid_search.best_score_)\n",
    "\n",
    "    xgb_best = grid_search.best_estimator_\n",
    "    predictions_xgb = xgb_best.predict(X_val)\n",
    "\n",
    "    print('MSE: {0:.3f}'.format(mean_squared_error(y_val, predictions_xgb)))\n",
    "    print('MAE: {0:.3f}'.format(mean_absolute_error(y_val, predictions_xgb)))\n",
    "    print('R^2: {0:.3f}'.format(r2_score(y_val, predictions_xgb)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next experiment with random forest\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [30, 50],\n",
    "    \"min_samples_split\": [5, 10, 20],\n",
    "\n",
    "}\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=1000)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "\n",
    "rf_best = grid_search.best_estimator_\n",
    "predictions_rf = rf_best.predict(X_val)\n",
    "\n",
    "print('MSE: {0:.3f}'.format(mean_squared_error(y_val, predictions_rf)))\n",
    "print('MAE: {0:.3f}'.format(mean_absolute_error(y_val, predictions_rf)))\n",
    "print('R^2: {0:.3f}'.format(r2_score(y_val, predictions_rf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05,0.1,0.2],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"colsample_bytree\": [0.7,0.8,0.9],\n",
    "}\n",
    "\n",
    "\n",
    "model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)\n",
    "\n",
    "grid_search = GridSearchCV(model_xgb, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "\n",
    "xgb_best = grid_search.best_estimator_\n",
    "predictions_xgb = xgb_best.predict(X_val)\n",
    "\n",
    "print('MSE: {0:.3f}'.format(mean_squared_error(y_val, predictions_xgb)))\n",
    "print('MAE: {0:.3f}'.format(mean_absolute_error(y_val, predictions_xgb)))\n",
    "print('R^2: {0:.3f}'.format(r2_score(y_val, predictions_xgb)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " xgb_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05,0.1,0.2],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"colsample_bytree\": [0.7,0.8,0.9],\n",
    "}\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "X_scaled_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)\n",
    "\n",
    "grid_search = GridSearchCV(model_xgb, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_scaled_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "\n",
    "xgb_best = grid_search.best_estimator_\n",
    "predictions_xgb = xgb_best.predict(X_scaled_val)\n",
    "\n",
    "print('MSE: {0:.3f}'.format(mean_squared_error(y_val, predictions_xgb)))\n",
    "print('MAE: {0:.3f}'.format(mean_absolute_error(y_val, predictions_xgb)))\n",
    "print('R^2: {0:.3f}'.format(r2_score(y_val, predictions_xgb)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_test = scaler.transform(X_test)\n",
    "xgb_best.predict(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weight of Features\n",
    "---\n",
    "\n",
    "Create a new pandas DataFrame with two features, X_train.columns and xgb_best.feature_importances_. By basic pandas operations, we can sketch out what the important features in the mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=pd.DataFrame()\n",
    "df_p['features']=X_train.columns\n",
    "df_p['importance']=xgb_best.feature_importances_\n",
    "\n",
    "df_pp=df_p[df_p.importance>0.01]\n",
    "df_pp=df_pp.sort_values(['importance'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "n=len(df_pp)\n",
    "plt.yticks(list(range(n)), df_pp.features, fontsize=18)\n",
    "plt.barh(df_pp.features,df_pp.importance)\n",
    "plt.title('0050', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function of Importance Visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImportance(xgb_best,features,etf='0050'):\n",
    "    df_p=pd.DataFrame()\n",
    "    df_p['features']=features\n",
    "    df_p['importance']=xgb_best.feature_importances_\n",
    "\n",
    "    df_pp=df_p[df_p.importance>0.01]\n",
    "    df_pp=df_pp.sort_values(['importance'],ascending=True)\n",
    "    fig,ax = plt.subplots(figsize=(15,15))\n",
    "    n=len(df_pp)\n",
    "    plt.yticks(list(range(n)), df_pp.features, fontsize=18)\n",
    "    plt.barh(df_pp.features,df_pp.importance)\n",
    "    plt.title(etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=X_train.columns\n",
    "plotImportance(xgb_best,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "ax.set_yticklabels(X_train.columns)\n",
    "\n",
    "plot_importance(xgb_best,height=0.5,ax=ax,max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date=pd.read_csv(\"yahoo/update20180518/00692.csv\",index_col='Date') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df0050_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsceme(df,size=150,etf='0050'):\n",
    "    if (len(df)>365):    \n",
    "       data=generate_features_p23(df)\n",
    "    else:\n",
    "       data=generate_features_p23_1(df) \n",
    "    X_columns = list(data.drop(['Close'], axis=1).columns)\n",
    "    y_column = 'Close'\n",
    "    #size=size\n",
    "    X = data[X_columns][-size:]\n",
    "    y = data[y_column][-size:]\n",
    "    #print(len(data))\n",
    "    val_ratio =0.2\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=val_ratio,random_state=42)\n",
    "    \n",
    "    param_grid = {\n",
    "       \"learning_rate\": [0.05,0.1,0.2],\n",
    "       \"max_depth\": [5, 10, 15, 20],\n",
    "       \"colsample_bytree\": [0.7,0.8,0.9],\n",
    "    }\n",
    "\n",
    "    model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)\n",
    "\n",
    "    grid_search = GridSearchCV(model_xgb, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "    # print(grid_search.best_score_)\n",
    "\n",
    "    xgb_best = grid_search.best_estimator_\n",
    "    predictions_xgb = xgb_best.predict(X_val)\n",
    "\n",
    "    print('MSE: {0:.3f}'.format(mean_squared_error(y_val, predictions_xgb)))\n",
    "    print('MAE: {0:.3f}'.format(mean_absolute_error(y_val, predictions_xgb)))\n",
    "    print('R^2: {0:.3f}'.format(r2_score(y_val, predictions_xgb)))\n",
    "\n",
    "    features=X_train.columns\n",
    "    plotImportance(xgb_best,features,etf=etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etfs=['0050','0051','0052','0053','0054','0055','0056','0057','0058',\n",
    "      '0059','006201','006203','006204','006208','00690','00692','00701','00713']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datadir=\"yahoo/update20180518\"\n",
    "\n",
    "for etf in  etfs:\n",
    "    \n",
    "    etf_name=datadir+\"/%s.csv\" %etf\n",
    "    df_tmp=pd.read_csv(etf_name,index_col='Date')\n",
    "    print(etf,'\\n----') \n",
    "    #df_tmp.info()\n",
    "    gridsceme(df_tmp,size=150,etf=etf)\n",
    "    del df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0050_date.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsceme(df0050_date,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor(learning_rate=0.1, random_state=3, n_estimators=90, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_xgb=model_xgb.predict(X)\n",
    "Y_pred_xgb[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length Calculation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dirname=\"yahoo/update20180518/\" \n",
    "\n",
    "print(\"Summary of ETF's\\n...\\n\")\n",
    "for file in os.listdir(dirname):\n",
    "    print(\"ETF-%s\" %file[:-4])\n",
    "    df=pd.read_csv(dirname+file)\n",
    "    l=len(df)\n",
    "    day1=df.Date[0]\n",
    "    day2=df.Date[l-1]\n",
    "    print(l,\" days include: begin from \",day1,\" to \",day2)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "dirname=\"yahoo/update20180518/\" \n",
    "\n",
    "print(\"Summary of ETF's\\n...\\n\")\n",
    "print('{:10}{:12}{:13}{:6}'.format(\"ETF_code\",\"Start_Date\",\"End_Date\",\" Days\"))\n",
    "for file in os.listdir(dirname):\n",
    "    #print(\"ETF-%s\" %file[:-4])\n",
    "    name=file[:-4]\n",
    "    df=pd.read_csv(dirname+file)\n",
    "    l=len(df)\n",
    "    day1=df.Date[0]\n",
    "    day2=df.Date[l-1]\n",
    "    print('{:1}{:9}{:12}{:12}{:6}'.format(\" \",name,day1,day2,l))\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekpre=\"yahoo/20180518pre30/\"\n",
    "\n",
    "plt.figure(figsize=(16,18))\n",
    "\n",
    "i=1\n",
    "for file in os.listdir(weekpre):\n",
    "    f=pd.read_csv(weekpre+file)\n",
    "    plt.subplot(6,3,i)\n",
    "    s=\"%s\" %file[:-3]\n",
    "    plt.plot(f.Close[-30:],label=s) \n",
    "    plt.legend(loc='upper right')\n",
    "    i=i+1\n",
    "#plt.legend(True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
